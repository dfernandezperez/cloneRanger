import pandas as pd
import yaml
from snakemake.utils import validate

configfile: "config/config.yaml"
container : "docker://condaforge/mambaforge:4.10.3-6"


#-----------------------------------------------------------------------------------------------------------------------
# Load sample sheet and cluster configuration, config file and validate structures
#-----------------------------------------------------------------------------------------------------------------------

# Load units
units       = pd.read_csv(config["units"], dtype = str, sep = "\t").set_index(["sample_id", "lib_type", "lane"], drop = False).sort_index()
units.index = units.index.set_levels([i.astype(str) for i in units.index.levels])  # enforce str in index

# Resources and cell hashing information yaml files
RESOURCES    = yaml.load(open(config['resources'], 'r'), Loader=yaml.FullLoader)
CELL_HASHING = yaml.load(open(config['cell_hashing'], 'r'), Loader=yaml.FullLoader)

# Validate structures
validate(units, schema = "schema/units.schema.yaml")
validate(config, schema = "schema/config.schema.yaml")
validate(CELL_HASHING, schema = "schema/cell_hashing.schema.yaml")

# Store sample names, larry colors and lib tyes in variables.
SAMPLES      = set(units["sample_id"])
LARRY_COLORS = [color for color in config["feature_bc_config"]["bc_patterns"].values()]
LIB_TYPES    = set(units["lib_type"])


#-----------------------------------------------------------------------------------------------------------------------
# Rules
#-----------------------------------------------------------------------------------------------------------------------
include: "rules/common.smk"
include: "rules/resources.smk"
include: "rules/counts.smk"
include: "rules/larry_processing.smk"
include: "rules/preprocessing.smk"

# Create a small dictionary with the library types corresponding to each sample. This is done in this way to be able to 
# execute the pipeline without larry (fb) or cellhashing also when those fastq files are present in the units.tsv file.
SAMPLE_LIB_DICT = {key: [] for key in SAMPLES}
for sample,lib_type in zip(units["sample_id"].values, units["lib_type"].values):
    if not is_feature_bc() and lib_type == "FB":
        next
    elif not is_cell_hashing(sample) and lib_type == "CH":
        next
    else:
        SAMPLE_LIB_DICT[sample].append(lib_type)

#-----------------------------------------------------------------------------------------------------------------------
# Rule all
#-----------------------------------------------------------------------------------------------------------------------
# Define target files based on pipeline
if config["10x_pipeline"] == "GEX" and is_feature_bc():
    target_files = [
        "results/04_RNA-exploration/RNA_exploration.html",
        expand("results/05_barcode-exploration/{sample}_barcode_filtering.html", sample = SAMPLES),
        "results/05_barcode-exploration/barcode_exploration.html"
        ]

elif config["10x_pipeline"] == "GEX" and not is_feature_bc():
    target_files = "results/04_RNA-exploration/RNA_exploration.html"
    
elif config["10x_pipeline"] in ["ARC", "ATAC"]:
    target_files = expand("results/01_counts/{sample}/outs/filtered_feature_bc_matrix.h5", sample = SAMPLES)

rule all:
    input:
        expand("results/02_createSeurat/seurat_{sample}_noDoublets.rds", sample = SAMPLES)